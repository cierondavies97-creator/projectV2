Realign Master System Spec (Engine + Research Machine)
0) One-sentence definition

A reproducible, auditable, multi-paradigm trading research and execution system that runs a deterministic microbatch engine (daily × instrument-cluster) and a separate research/training lane (Bayes/GA/RFE/portfolio evaluation) to generate, test, and promote strategies (“paradigms”) as configuration + focused logic, not hard-wired rules.

1) Non-negotiable design goals
1.1 Multi-paradigm by construction

The system must support many strategy families equally (ICT, stat-TS, mean reversion, cross-sectional RV, vol/range, meta/ensemble, microstructure, etc.).

No paradigm is “special” in code structure, data model, or orchestration.

A “paradigm” is:

Config (parameters, feature set, thresholds, constraints, assets/universe)

Focused logic (hypothesis generation + scoring + gating hooks)

Stable artifacts (hypotheses, critic, decisions, portfolio, trade paths)

1.2 Hard separation of lanes (strong boundaries)

Deterministic engine lane: pure, replayable, no learning, no mutation of live configs.

Research/training lane: Bayes/GA/RFE/Monte Carlo/Markov, hyperparam search, feature selection, evaluation.

Control-plane/tools lane: orchestration scripts, config management, run scheduling, reporting, governance.

1.3 Reproducibility and auditability

Every run must be reproducible from:

immutable inputs (data snapshot + configs)

deterministic RNG policy (seeded via RunContext)

versioned code (Git commit)

stable artifact schemas (Parquet source-of-truth)

1.4 Explicit, observable, debuggable

Every step writes artifacts and metrics.

Every decision is explainable via stored intermediate columns, scores, and gating reasons.

No “hidden state” inside long-lived objects.

2) Canonical identity model (required on all artifacts)

All runs and outputs must be keyed by a RunContext identity model:

Required identifiers

env: dev/prod/research

mode: backtest/paper/live

snapshot_id: immutable input snapshot identifier (data + configs)

run_id: unique per execution run

experiment_id: research grouping id (optional in engine, required in research)

candidate_id: strategy candidate id (required for research/training outputs)

paradigm_id: which paradigm family/config generated the hypothesis/decision

principle_id: optional sub-logic identifier inside a paradigm (rule family, setup type)

RNG policy

Central RNG policy derived from (base_seed, snapshot_id, run_id, instrument_cluster_id, dt, step_name) so that:

microbatch is deterministic

research can create controlled stochasticity with explicit seeds

3) Data & storage conventions
3.1 Data formats

Parquet is the source of truth for artifacts and datasets.

Polars is the default dataframe engine.

DuckDB is allowed as a query layer (read-only convenience), not as the canonical store.

3.2 Partitioning conventions

Partition by the keys needed for slicing, reproducibility, and incremental runs:

engine artifacts: (mode, run_id, instrument, dt) plus paradigm_id where relevant

research artifacts: (experiment_id, candidate_id, dt) plus universe partitioning if needed

3.3 Directory layout (canonical)

A minimal stable layout that supports comparison across time/paradigms:

conf/
  retail.yaml                 # global runtime config and universe metadata (read-only in runs)
  paradigms/                  # paradigm configs (YAML)
data/
  candles/                    # OHLCV / bar data (partitioned)
  ticks/                      # tick/L1 if available
  features/                   # optional cached features (if strictly deterministic + versioned)
artifacts/
  engine/
    hypotheses/
    critic/
    decisions/
    orders/
    fills/
    trade_paths/
    reports/
  research/
    feature_scores/
    bayes/
    ga/
    rfe/
    portfolio/
docs/
  MASTER_SYSTEM_SPEC.md       # this doc
src/
  engine/
  research/
  control_plane/
scripts/


Control-plane rule: engine runs must never silently change conf/ or live configs. Any config changes occur via explicit tooling and result in a new snapshot_id.

4) Engine: microbatch pipeline (deterministic lane)
4.1 Microbatch definition

A microbatch is: one trading day × one instrument cluster
Entry-point: run_microbatch(ctx, key) where key = (dt, instrument_cluster_id).

4.2 BatchState (shared state object)

A single BatchState flows through ordered steps. It must be:

immutable-by-default pattern (copy-on-write or explicit .with_*() methods)

serializable references to artifact paths

strict about required columns per stage

4.3 Canonical ordered steps

The engine runs the same ordered step chain for every paradigm, with paradigm-specific logic plugged into defined extension points:

ingest

load bars/ticks for the cluster/day

attach universe metadata (pip size, min tick, session calendar)

features

build deterministic features (HTF/LTF, microstructure where available)

windows

construct analysis windows (sessions, ranges, swing windows, volatility regimes)

hypotheses

generate candidate hypotheses/trade ideas (multi-paradigm)

critic

score and explain; attach risk, regime fit, constraint checks

pretrade

transform accepted hypotheses into actionable intents (entry/SL/TP/brackets)

gatekeeper

final eligibility filter + context selection (risk limits, correlation, schedule, liquidity)

portfolio

position sizing and cross-sectional allocation

brackets

finalize order instructions and bracket logic

reports

write summary metrics, diagnostics, traceability outputs

4.4 Step contracts (required)

Each step must have:

required_inputs: artifact references and minimum column sets

produces: artifact schema + partition keys

deterministic behavior given (ctx, key)

structured logging with run identifiers

4.5 Deterministic pipeline contract (system-wide)

This is the enforceable contract for every engine-aligned module in the system (engine steps, scripts, and any code that writes engine artifacts).

Contract invariants (non-negotiable):

- Single-writer rule: each artifact/table has exactly one owning step that writes it.
- Deterministic by default: behavior is a pure function of (RunContext, MicrobatchKey, immutable inputs).
- No hidden state: step functions cannot depend on global mutable state or implicit caches.
- Strict schema discipline: add columns only in backward-compatible ways, with explicit nullability rules.
- Fail-fast validation: if a required column is missing or null where required, the step must raise and abort the microbatch.
- Full provenance: every output row carries RunContext keys and microbatch identity.
- Stable partitioning: artifact partitioning must follow the identity model and never change implicitly.
- Observability: each step writes metrics and manifests (row counts, input hashes, output paths).

Step interface contract (minimum):

- Inputs: required tables + minimum columns + config namespaces (explicit list per step).
- Outputs: persisted table(s) + schema + partition keys + ownership record.
- Deterministic RNG: any randomness must be derived from the canonical seed policy.
- Audit trail: a step must be traceable through logging + artifact manifests.

Compliance requirements:

- Any new step, or any change to an existing step, must update the IO contract + schema registry and add golden tests where deterministic outputs are required.
- Steps must not mutate configs; any changes to conf/ must be explicit, reviewable, and bound to a new snapshot_id.

4.6 Phase B compliance requirements (deterministic lane)

Phase B is a strict engine contract. To be considered compliant, the deterministic pipeline must:

- Write canonical Phase B fields into data/windows on every anchor row (dr_id, dr_phase, dr_low/high/mid/width, dr_age_bars, dr_start_ts, dr_last_update_ts, pd_index, range_position, counters, reason codes).
- Emit an event table (data/market_events or data/dealing_range_events) with one row per detected structural event and a stable event_type enum.
- Use a deterministic, stateless event detector + state-machine fold to assign dr_phase and dr_reason_code.
- Treat windows as the single source of truth for dr_phase; downstream steps may consume but must not recompute it.
- Fail fast if Phase B-required fields are missing or null where required.
- Maintain golden fixtures for each anchor timeframe to protect phase-label stability.

4.7 Connected scripts & tooling that must be updated when the pipeline contract changes

These scripts/tools are tightly coupled to the deterministic pipeline contract and must be audited for updates whenever steps, schemas, or ownership rules change:

- scripts/run_microbatch.py: primary microbatch entrypoint and summary reporting.
- scripts/run_microbatch.ps1: PowerShell wrapper for run_microbatch.py and CLI arg compatibility.
- scripts/audit_io_contract.py: validates that write ownership matches the IO contract and step call-sites.
- scripts/compile_features_registry_contract.py: enforces schema registration expectations for features/windows.
- scripts/check_anchor_grid.py: validates anchor grid assumptions for windows and downstream joins.
- scripts/smoke_validate_parquet.py: validates Parquet outputs; should reflect any new schemas or partitions.
- scripts/dev_inspect_*: developer inspection tools for step artifacts (features/windows/hypotheses/critic/etc.).
- scripts/run_ingest.py / scripts/run_features.py / scripts/run_reports.py: currently empty placeholders; must be implemented or removed to avoid ambiguity when the pipeline is rewritten.

5) Paradigm interface (how “multi-paradigm” is implemented)
5.1 Paradigm = config + plugin logic

A paradigm contributes:

Config: parameters, universe, timeframes, windows, feature toggles

Hypothesis generator: creates candidate hypotheses in normalized schema

Critic hooks: optional additional scores/filters

Pretrade mapping: entry model mapping (market/limit/stop logic)

Portfolio hooks: optional factor exposures / risk budgets (still in normalized interface)

5.2 No paradigm-specific pipelines

The pipeline order is fixed. Paradigms cannot reorder steps; they only implement hooks.

5.3 Normalized hypothesis model (engine artifact)

Every hypothesis is a row with:

identity: snapshot_id, run_id, dt, instrument, instrument_cluster_id, paradigm_id, principle_id, hypothesis_id

time/context: anchor_tf, entry_tf, window_id, event_ts, session_bucket, dow_bucket, tod_bucket

directionality: side (long/short), optionally bias_strength

proposal: entry_px, sl_px, tp_px or sufficient info to derive them deterministically

explainability: feature references, tags, and a params_json / structured column

5.4 Normalized critic model (engine artifact)

Every critic row must include:

hypothesis_id join key + all identity keys

critic_score_total and named component scores (e.g., score_regime, score_structure, score_risk, score_costs)

gating flags + reasons (string enums): reject_reason_primary, reject_reasons_all

derived risk/cost estimates: spread/slippage estimates, expected excursion stats if available

6) Research machine (training lane): Bayes + GA + RFE + portfolio evaluation
6.1 The research loop (conceptual)

Research consumes engine artifacts (hypotheses/critic/decisions/trade_paths) and produces improved candidates/configs:

Collect training sets

join outcomes to hypotheses and contexts (including microstructure datasets where available)

Feature evaluation

score features, thresholds, stability across regimes, leakage checks

Candidate generation

GA grammar generates rule structures / hypothesis templates

RFE enumerates and prunes recipes (feature subsets + rule components)

Parameter learning

Bayes: threshold/posterior learning for decision rules, gates, or scoring weights

Portfolio layer evaluation

cross-sectional construction, correlation control, risk budgets

Promotion

produce a new candidate config bundle => new candidate_id and (when promoted) new snapshot_id

6.2 Bayes training (requirements)

Bayesian components are used for:

threshold learning (feature → decision boundary)

score calibration (critic component weighting/posterior)

regime-conditional parameterization (different posteriors per context bucket)

Outputs must be:

versioned posterior summaries

explicit priors

dataset hash / snapshot references

calibration diagnostics

6.3 GA grammar training (requirements)

GA is used to evolve:

hypothesis rule structures (“grammar”)

logical compositions of features/windows/conditions

parameter sets encoded as genes with explicit bounds

GA outputs must include:

full candidate genome

decoded human-readable rule spec

fitness breakdown (return, drawdown, stability, turnover, costs, regime robustness)

seeds and reproducibility metadata

6.4 RFE (recipe/rule enumerator) (requirements)

RFE enumerates “recipes”:

feature subsets

rule components

gating variants

window definitions

RFE must produce:

ranked recipes

ablation results

sensitivity to timeframe and regime

overfit controls (walk-forward, embargo, clustering by instrument/time)

6.5 Portfolio research layer (requirements)

Cross-sectional portfolio logic is a first-class component:

allocations across instruments/cluster

correlation and exposure control

risk budgeting and leverage caps

capacity and cost constraints (spread/slippage models)

Portfolio outputs must be comparable across candidates via standardized metrics.

7) Gatekeeper & context selection (engine + research-critical)

Gatekeeper is the final enforcement point before orders:

selects the active context/regime interpretation for the day/instrument

enforces hard risk and operational constraints

filters/limits correlated bets

ensures “explainable rejection”: every reject must have a stored reason code

Context selection must be:

deterministic in engine lane

learnable in research lane (e.g., posterior over regimes), but only promoted via config snapshots

8) Trade paths and evaluation artifacts (must exist)
8.1 Trade path artifact (canonical)

A normalized table that records the full lifecycle:

ids: snapshot_id, run_id, dt, trade_id, instrument, paradigm_id, principle_id

entry/exit: entry_ts, entry_px, exit_ts, exit_px

bracket: tp_px, sl_px

context: time buckets, windows, regime labels

outcome: return, MAE/MFE, duration, costs (explicit)

8.2 Research evaluation staples

Every candidate evaluation must include:

walk-forward or out-of-sample split metadata

stability across regimes and instruments

turnover and cost-adjusted performance

failure mode clustering (why it loses, where it breaks)

9) Governance & internal controls (LLM-safe and operator-safe)
9.1 Control-plane safety rule

The LLM must never silently mutate:

live configs

production parameters

execution toggles

All changes must be:

proposed as diffs

reviewed/approved by the operator

materialized as a new snapshot/config bundle

9.2 Audit trail requirements

Every run must record:

code version (git commit)

config snapshot ids and hashes

dataset hashes (or partition lists)

step-level artifact manifests

10) Implementation defaults (technical constraints)

Python monolith, CPU-oriented.

NumPy/SciPy + Polars + Parquet as primary stack; DuckDB allowed as query convenience.

Deterministic engine microbatches with strict step ordering.

Research lane may be stochastic but must be reproducible via explicit seeds and stored metadata.

Strong schema discipline: add columns in backward-compatible ways; never break joins.

11) What “done” looks like (final target design)

A single operator can:

Run deterministic microbatches across many days/instrument clusters.

Inspect hypotheses → critic → decisions → portfolio → trade_paths with full traceability.

Run Bayes training to calibrate thresholds and critic weights.

Run GA grammar to discover rule structures across paradigms.

Run RFE enumeration to prune and validate feature/rule recipes.

Evaluate candidates through the portfolio layer with standardized metrics.

Promote a candidate to a new config snapshot safely (no silent live mutation).

Repeat across paradigms without changing the core engine pipeline.

12) Guidance to the next LLM (Codex/GitHub)

When updating this repo, prioritize:

preserving deterministic engine contracts

improving observability (schemas, logs, reports)

adding paradigms as plugins/config + focused hooks (not new pipelines)

keeping artifacts comparable across time and across paradigms

never editing live configs without explicit operator-approved diffs

If any handbook text or legacy notes conflict with the above principles:

flag the conflict explicitly

propose the better design

keep backward compatibility in artifacts whenever possible
